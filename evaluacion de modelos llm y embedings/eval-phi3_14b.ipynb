{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9428bcf-62b4-4f23-8793-57dd8cccc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacado de la documentación, yo no lo he ejecutado.\n",
    "# %pip install llama-index-llms-openai llama-index-embeddings-openai\n",
    "\n",
    "# estos son los módulos que tengo instalados\n",
    "# pip install llama-index\n",
    "# pip install llama-index-embeddings-huggingface\n",
    "# pip install llama-index-llms-ollama\n",
    "# pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dee1ca-879e-4ae8-8efc-08d07f2a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a14b3e-dceb-417d-8813-c88cb88a0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"TU KEY\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2f4f7f-1c0b-4812-b88f-c0a0ef51c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    GuidelineEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec67b1ad-66b2-4820-8397-3099989578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "gpt3_5 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "faithfulness_gpt3_5 = FaithfulnessEvaluator(llm=gpt3_5)\n",
    "relevancy_gpt3_5 = RelevancyEvaluator(llm=gpt3_5)\n",
    "correctness_gpt3_5 = CorrectnessEvaluator(llm=gpt3_5)\n",
    "semantic = SemanticSimilarityEvaluator()  # coge el modelo de embedings de settings si no se le pasa ninguno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a222d30-516d-4a02-876a-5cf9698f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aquí crear el modelo que quieras evaluar\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"phi3:14b\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f068f61-6377-4c95-8643-e74854884cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_diccionarios(dict1, dict2):\n",
    "    resultado = {}\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            # Combina los arrays de ambas claves en un solo array\n",
    "            resultado[key] = dict1[key] + dict2[key]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604b1d27-9fbe-41e2-bd7a-0aed06f3a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he modificado la del ejemplo para que use result.score en vez de result.passing. en algunos tipos de evaluación sale un resultado más preciso\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    sum_score = 0\n",
    "    count = 0\n",
    "    for result in results:\n",
    "        if result.score is not None:  # Verifica si score no es None\n",
    "            sum_score += result.score\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        print(f\"{key} Score: No valid scores\")\n",
    "        return None\n",
    "    score = sum_score / count\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29194fda-1572-48e6-880a-a6623d6b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of correctcness results: 20\n",
      "results of datasets paul_graham of model: phi3:14b\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 3.966666666666667\n",
      "semantic Score: 0.9462366312576632\n",
      "length of correctcness results: 40\n",
      "results of datasets Blockchain of model: phi3:14b\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.85\n",
      "correctness Score: 4.09375\n",
      "semantic Score: 0.9449369574724695\n",
      "length of correctcness results: 60\n",
      "results of datasets Alexnet of model: phi3:14b\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.15625\n",
      "semantic Score: 0.9425194730420973\n",
      "length of correctcness results: 80\n",
      "results of datasets Covid of model: phi3:14b\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.131578947368421\n",
      "semantic Score: 0.9467355054952173\n",
      "length of correctcness results: 100\n",
      "results of datasets Llama2Paper of model: phi3:14b\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.85\n",
      "correctness Score: 4.138888888888889\n",
      "semantic Score: 0.9392569063489835\n",
      "total results of all datasets of model: phi3:14b\n",
      "faithfulness Score: 0.93\n",
      "relevancy Score: 0.88\n",
      "correctness Score: 4.101190476190476\n",
      "semantic Score: 0.9439370947232866\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset, LabelledRagDataset\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "datasets = [\"paul_graham\", \"Blockchain\", \"Alexnet\", \"Covid\", \"Llama2Paper\"]\n",
    "total_results = {\"correctness\": [], \"faithfulness\": [], \"relevancy\": [], \"semantic\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    path_dataset = f\"./eval_data/rag_data/{datasets[0]}/rag_dataset.json\"\n",
    "    path_documents = f\"./eval_data/rag_data/{datasets[0]}/source_files\"\n",
    "    rag_dataset = LabelledRagDataset.from_json(path_dataset)\n",
    "    documents = SimpleDirectoryReader(input_dir=path_documents).load_data()\n",
    "\n",
    "    rag_dataset_pandas = rag_dataset.to_pandas()\n",
    "    queries = rag_dataset_pandas[\"query\"]\n",
    "    reference_answers = rag_dataset_pandas[\"reference_answer\"]\n",
    "    \n",
    "    queries = queries[:20]\n",
    "    reference_answers = reference_answers[:20].to_list()\n",
    "\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, transformations=[splitter]\n",
    "    )\n",
    "\n",
    "    # ponemos todos los evaluadores que queremos usar aquí.\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"correctness\": correctness_gpt3_5,\n",
    "         \"faithfulness\": faithfulness_gpt3_5,\n",
    "         \"relevancy\": relevancy_gpt3_5,\n",
    "         \"semantic\": semantic},\n",
    "        workers=8,\n",
    "    )\n",
    "    \n",
    "    # he corregido una cosa de aquí del ejemplo, que no estaba (llm=llm), supongo que por error.\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        vector_index.as_query_engine(llm=llm),\n",
    "        queries=queries,\n",
    "        reference=reference_answers,\n",
    "    )\n",
    "    \n",
    "    total_results = combinar_diccionarios(total_results, eval_results)\n",
    "    print(f\"length of correctcness results: {len(total_results[\"correctness\"])}\")\n",
    "\n",
    "    print(f\"results of datasets {dataset_name} of model: {llm.model}\")\n",
    "    score_faithfulness = get_eval_results(\"faithfulness\", eval_results)\n",
    "    score_relevancy = get_eval_results(\"relevancy\", eval_results)\n",
    "    score_correctness = get_eval_results(\"correctness\", eval_results)\n",
    "    score_semantic = get_eval_results(\"semantic\", eval_results)\n",
    "\n",
    "print(f\"total results of all datasets of model: {llm.model}\")\n",
    "score_faithfulness = get_eval_results(\"faithfulness\", total_results)\n",
    "score_relevancy = get_eval_results(\"relevancy\", total_results)\n",
    "score_correctness = get_eval_results(\"correctness\", total_results)\n",
    "score_semantic = get_eval_results(\"semantic\", total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce1c4d1-d3bd-4faa-bfe3-116501df2b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['correctness', 'faithfulness', 'relevancy', 'semantic'])\n",
      "dict_keys(['query', 'contexts', 'response', 'passing', 'feedback', 'score', 'pairwise_source', 'invalid_result', 'invalid_reason'])\n",
      "False\n",
      "The author's initial encounter with computers was through a TRS-80 model, which they procured around 1980 despite their expense at the time. This device served as his primary tool for programming during this period, allowing him to write games and utility programs like a rocket flight predictor and a rudimentary word processor. However, due to limited memory capacity (approximately equivalent to two pages of text), he was compelled to segment his writing process into chunks before printing it out - an improvement over using typewriters but still restrictive.\n",
      "\n",
      "For coding on the TRS-80, the author utilized a language called Fortran. The programming process involved typing programs onto punch cards and loading them into memory via stacking in a card reader. Program execution would usually result in printed output from a loud printer - an experience likely marked by anticipation and surprise, given the lack of real-time feedback mechanisms present today.\n",
      "\n",
      "A particular challenge he faced was understanding the functionality of the IBM 1401, which his school district used for data processing tasks. The author struggled to find suitable input or tasks that didn't rely on predefined data stored in punch cards - a limitation inherent in systems like the IBM 1401 during this time period.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(eval_results.keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].dict().keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].passing)\n",
    "print(eval_results[\"correctness\"][0].response)\n",
    "print(eval_results[\"correctness\"][0].contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e8f924-9e46-4b80-b4c1-3c352209567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvaluationResult(query='In the essay, the author mentions his early experiences with programming. Describe the first computer he used for programming, the language he used, and the challenges he faced.', contexts=None, response=\"The author's initial encounter with computers was through a TRS-80 model, which they procured around 1980 despite their expense at the time. This device served as his primary tool for programming during this period, allowing him to write games and utility programs like a rocket flight predictor and a rudimentary word processor. However, due to limited memory capacity (approximately equivalent to two pages of text), he was compelled to segment his writing process into chunks before printing it out - an improvement over using typewriters but still restrictive.\\n\\nFor coding on the TRS-80, the author utilized a language called Fortran. The programming process involved typing programs onto punch cards and loading them into memory via stacking in a card reader. Program execution would usually result in printed output from a loud printer - an experience likely marked by anticipation and surprise, given the lack of real-time feedback mechanisms present today.\\n\\nA particular challenge he faced was understanding the functionality of the IBM 1401, which his school district used for data processing tasks. The author struggled to find suitable input or tasks that didn't rely on predefined data stored in punch cards - a limitation inherent in systems like the IBM 1401 during this time period.\", passing=False, feedback=\"The generated answer is relevant to the user query as it discusses the author's early experiences with programming, the first computer he used, the programming language, and the challenges faced. However, there are some inaccuracies such as mentioning the TRS-80 model instead of the IBM 1401 as the first computer used for programming, and inaccuracies in the details of the programming process and challenges faced. Overall, the answer provides relevant information but contains mistakes that lower its score.\", score=3.5, pairwise_source=None, invalid_result=False, invalid_reason=None)]\n"
     ]
    }
   ],
   "source": [
    "print(eval_results[\"correctness\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cabe1-3c56-4cd9-8da6-b051537a660a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b378de-c3af-4e8d-9b43-9b110efaa550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3f2e-4bf5-4acb-8d7c-0e2289aaecb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a478b-b060-4288-b479-dba8fa694680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bc6dc-7a82-48a9-94f5-da7b0d906da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
