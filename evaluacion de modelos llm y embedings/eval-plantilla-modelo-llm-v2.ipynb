{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9428bcf-62b4-4f23-8793-57dd8cccc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacado de la documentación, yo no lo he ejecutado.\n",
    "# %pip install llama-index-llms-openai llama-index-embeddings-openai\n",
    "\n",
    "# estos son los módulos que tengo instalados\n",
    "# pip install llama-index\n",
    "# pip install llama-index-embeddings-huggingface\n",
    "# pip install llama-index-llms-ollama\n",
    "# pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8dee1ca-879e-4ae8-8efc-08d07f2a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a14b3e-dceb-417d-8813-c88cb88a0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"TU KEY\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2f4f7f-1c0b-4812-b88f-c0a0ef51c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    GuidelineEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec67b1ad-66b2-4820-8397-3099989578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "gpt3_5 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "faithfulness_gpt3_5 = FaithfulnessEvaluator(llm=gpt3_5)\n",
    "relevancy_gpt3_5 = RelevancyEvaluator(llm=gpt3_5)\n",
    "correctness_gpt3_5 = CorrectnessEvaluator(llm=gpt3_5)\n",
    "semantic = SemanticSimilarityEvaluator()  # coge el modelo de embedings de settings si no se le pasa ninguno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a222d30-516d-4a02-876a-5cf9698f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aquí crear el modelo que quieras evaluar\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"example\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f068f61-6377-4c95-8643-e74854884cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_diccionarios(dict1, dict2):\n",
    "    resultado = {}\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            # Combina los arrays de ambas claves en un solo array\n",
    "            resultado[key] = dict1[key] + dict2[key]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604b1d27-9fbe-41e2-bd7a-0aed06f3a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he modificado la del ejemplo para que use result.score en vez de result.passing. en algunos tipos de evaluación sale un resultado más preciso\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    sum_score = 0\n",
    "    count = 0\n",
    "    for result in results:\n",
    "        if result.score is not None:  # Verifica si score no es None\n",
    "            sum_score += result.score\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        print(f\"{key} Score: No valid scores\")\n",
    "        return None\n",
    "    score = sum_score / count\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29194fda-1572-48e6-880a-a6623d6b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of correctcness results: 20\n",
      "results of datasets paul_graham of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.0\n",
      "semantic Score: 0.9499504179380726\n",
      "length of correctcness results: 40\n",
      "results of datasets Blockchain of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.95\n",
      "correctness Score: 4.033333333333333\n",
      "semantic Score: 0.9505754647071324\n",
      "length of correctcness results: 60\n",
      "results of datasets Alexnet of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.15625\n",
      "semantic Score: 0.9522501726086967\n",
      "length of correctcness results: 80\n",
      "results of datasets Covid of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.95\n",
      "correctness Score: 4.0\n",
      "semantic Score: 0.9517538945575236\n",
      "length of correctcness results: 100\n",
      "results of datasets Llama2Paper of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.076923076923077\n",
      "semantic Score: 0.9472889820347625\n",
      "total results of all datasets of model: example\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.92\n",
      "correctness Score: 4.054794520547945\n",
      "semantic Score: 0.9503637863692376\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset, LabelledRagDataset\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "datasets = [\"paul_graham\", \"Blockchain\", \"Alexnet\", \"Covid\", \"Llama2Paper\"]\n",
    "total_results = {\"correctness\": [], \"faithfulness\": [], \"relevancy\": [], \"semantic\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    path_dataset = f\"./eval_data/rag_data/{datasets[0]}/rag_dataset.json\"\n",
    "    path_documents = f\"./eval_data/rag_data/{datasets[0]}/source_files\"\n",
    "    rag_dataset = LabelledRagDataset.from_json(path_dataset)\n",
    "    documents = SimpleDirectoryReader(input_dir=path_documents).load_data()\n",
    "\n",
    "    rag_dataset_pandas = rag_dataset.to_pandas()\n",
    "    queries = rag_dataset_pandas[\"query\"]\n",
    "    reference_answers = rag_dataset_pandas[\"reference_answer\"]\n",
    "    \n",
    "    queries = queries[:20]\n",
    "    reference_answers = reference_answers[:20].to_list()\n",
    "\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, transformations=[splitter]\n",
    "    )\n",
    "\n",
    "    # ponemos todos los evaluadores que queremos usar aquí.\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"correctness\": correctness_gpt3_5,\n",
    "         \"faithfulness\": faithfulness_gpt3_5,\n",
    "         \"relevancy\": relevancy_gpt3_5,\n",
    "         \"semantic\": semantic},\n",
    "        workers=8,\n",
    "    )\n",
    "    \n",
    "    # he corregido una cosa de aquí del ejemplo, que no estaba (llm=llm), supongo que por error.\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        vector_index.as_query_engine(llm=llm),\n",
    "        queries=queries,\n",
    "        reference=reference_answers,\n",
    "    )\n",
    "    \n",
    "    total_results = combinar_diccionarios(total_results, eval_results)\n",
    "    print(f\"length of correctcness results: {len(total_results[\"correctness\"])}\")\n",
    "\n",
    "    print(f\"results of datasets {dataset_name} of model: {llm.model}\")\n",
    "    score_faithfulness = get_eval_results(\"faithfulness\", eval_results)\n",
    "    score_relevancy = get_eval_results(\"relevancy\", eval_results)\n",
    "    score_correctness = get_eval_results(\"correctness\", eval_results)\n",
    "    score_semantic = get_eval_results(\"semantic\", eval_results)\n",
    "\n",
    "print(f\"total results of all datasets of model: {llm.model}\")\n",
    "score_faithfulness = get_eval_results(\"faithfulness\", total_results)\n",
    "score_relevancy = get_eval_results(\"relevancy\", total_results)\n",
    "score_correctness = get_eval_results(\"correctness\", total_results)\n",
    "score_semantic = get_eval_results(\"semantic\", total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce1c4d1-d3bd-4faa-bfe3-116501df2b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['correctness', 'faithfulness', 'relevancy', 'semantic'])\n",
      "dict_keys(['query', 'contexts', 'response', 'passing', 'feedback', 'score', 'pairwise_source', 'invalid_result', 'invalid_reason'])\n",
      "True\n",
      "The author's first experience with programming was on an IBM 1401 in the basement of his junior high school when he was around 13 or 14 years old. He and a friend, Rich Draves, got permission to use it. The language they used for programming at that time was an early version of Fortran.\n",
      "\n",
      "However, due to limitations, the author faced challenges with using this computer for programming. First, there was no input data available on punched cards. Therefore, he could only create programs that didn't rely on any input, such as calculating approximations of pi but couldn't do anything interesting with it because he lacked sufficient math knowledge.\n",
      "\n",
      "Another challenge he faced was understanding what to do with the 1401. This suggests that the computer's interface and functionality were unfamiliar to him at that time. A significant issue arose when one of his programs didn't terminate, causing a social as well as technical problem since there was no time-sharing on this machine.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(eval_results.keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].dict().keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].passing)\n",
    "print(eval_results[\"correctness\"][0].response)\n",
    "print(eval_results[\"correctness\"][0].contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e8f924-9e46-4b80-b4c1-3c352209567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvaluationResult(query='In the essay, the author mentions his early experiences with programming. Describe the first computer he used for programming, the language he used, and the challenges he faced.', contexts=None, response=\"The author's first experience with programming was on an IBM 1401 in the basement of his junior high school when he was around 13 or 14 years old. He and a friend, Rich Draves, got permission to use it. The language they used for programming at that time was an early version of Fortran.\\n\\nHowever, due to limitations, the author faced challenges with using this computer for programming. First, there was no input data available on punched cards. Therefore, he could only create programs that didn't rely on any input, such as calculating approximations of pi but couldn't do anything interesting with it because he lacked sufficient math knowledge.\\n\\nAnother challenge he faced was understanding what to do with the 1401. This suggests that the computer's interface and functionality were unfamiliar to him at that time. A significant issue arose when one of his programs didn't terminate, causing a social as well as technical problem since there was no time-sharing on this machine.\", passing=True, feedback=\"The generated answer provides a detailed description of the author's early experiences with programming, including the first computer used (IBM 1401), the programming language (Fortran), and the challenges faced. The information is accurate and relevant to the user query, with additional insights into the challenges faced by the author. Minor improvements could be made in terms of conciseness, but overall, the answer is comprehensive and on point.\", score=4.5, pairwise_source=None, invalid_result=False, invalid_reason=None)]\n"
     ]
    }
   ],
   "source": [
    "print(eval_results[\"correctness\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cabe1-3c56-4cd9-8da6-b051537a660a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b378de-c3af-4e8d-9b43-9b110efaa550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3f2e-4bf5-4acb-8d7c-0e2289aaecb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a478b-b060-4288-b479-dba8fa694680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bc6dc-7a82-48a9-94f5-da7b0d906da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
