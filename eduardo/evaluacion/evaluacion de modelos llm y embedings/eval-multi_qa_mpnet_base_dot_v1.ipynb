{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9428bcf-62b4-4f23-8793-57dd8cccc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacado de la documentación, yo no lo he ejecutado.\n",
    "# %pip install llama-index-llms-openai llama-index-embeddings-openai\n",
    "\n",
    "# estos son los módulos que tengo instalados\n",
    "# pip install llama-index\n",
    "# pip install llama-index-embeddings-huggingface\n",
    "# pip install llama-index-llms-ollama\n",
    "# pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dee1ca-879e-4ae8-8efc-08d07f2a695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the same event-loop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a14b3e-dceb-417d-8813-c88cb88a0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"TU KEY\"\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2f4f7f-1c0b-4812-b88f-c0a0ef51c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    GuidelineEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec67b1ad-66b2-4820-8397-3099989578b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-3.5-turbo\n",
    "gpt3_5 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "faithfulness_gpt3_5 = FaithfulnessEvaluator(llm=gpt3_5)\n",
    "relevancy_gpt3_5 = RelevancyEvaluator(llm=gpt3_5)\n",
    "correctness_gpt3_5 = CorrectnessEvaluator(llm=gpt3_5)\n",
    "semantic = SemanticSimilarityEvaluator()  # coge el modelo de embedings de settings si no se le pasa ninguno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56fcc41-9a74-4fe1-a541-5c338e8209df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test_1/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# creo el modelo de embeddings que quiero probar\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40846c46-16e4-40ab-9453-fb966c6a4221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/multi-qa-mpnet-base-dot-v1\n"
     ]
    }
   ],
   "source": [
    "print(Settings.embed_model.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a222d30-516d-4a02-876a-5cf9698f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aquí crear el modelo que quieras evaluar\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3_1_sauerkraut\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f068f61-6377-4c95-8643-e74854884cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_diccionarios(dict1, dict2):\n",
    "    resultado = {}\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            # Combina los arrays de ambas claves en un solo array\n",
    "            resultado[key] = dict1[key] + dict2[key]\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604b1d27-9fbe-41e2-bd7a-0aed06f3a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he modificado la del ejemplo para que use result.score en vez de result.passing. en algunos tipos de evaluación sale un resultado más preciso\n",
    "def get_eval_results(key, eval_results):\n",
    "    results = eval_results[key]\n",
    "    sum_score = 0\n",
    "    count = 0\n",
    "    for result in results:\n",
    "        if result.score is not None:  # Verifica si score no es None\n",
    "            sum_score += result.score\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        print(f\"{key} Score: No valid scores\")\n",
    "        return None\n",
    "    score = sum_score / count\n",
    "    print(f\"{key} Score: {score}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29194fda-1572-48e6-880a-a6623d6b53a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of correctcness results: 20\n",
      "results of datasets paul_graham of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.95\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 4.0\n",
      "semantic Score: 0.9483131462711073\n",
      "length of correctcness results: 40\n",
      "results of datasets Blockchain of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.75\n",
      "correctness Score: 4.0\n",
      "semantic Score: 0.9429335376457614\n",
      "length of correctcness results: 60\n",
      "results of datasets Alexnet of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.9\n",
      "correctness Score: 3.9\n",
      "semantic Score: 0.9441007259660952\n",
      "length of correctcness results: 80\n",
      "results of datasets Covid of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.8\n",
      "correctness Score: 4.029411764705882\n",
      "semantic Score: 0.945689379815042\n",
      "length of correctcness results: 100\n",
      "results of datasets Llama2Paper of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.9\n",
      "relevancy Score: 0.75\n",
      "correctness Score: 4.0\n",
      "semantic Score: 0.9457478366289734\n",
      "total results of all datasets of embeddings model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "faithfulness Score: 0.91\n",
      "relevancy Score: 0.82\n",
      "correctness Score: 3.983695652173913\n",
      "semantic Score: 0.9453569252653959\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset, LabelledRagDataset\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "datasets = [\"paul_graham\", \"Blockchain\", \"Alexnet\", \"Covid\", \"Llama2Paper\"]\n",
    "total_results = {\"correctness\": [], \"faithfulness\": [], \"relevancy\": [], \"semantic\": []}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    path_dataset = f\"./eval_data/rag_data/{datasets[0]}/rag_dataset.json\"\n",
    "    path_documents = f\"./eval_data/rag_data/{datasets[0]}/source_files\"\n",
    "    rag_dataset = LabelledRagDataset.from_json(path_dataset)\n",
    "    documents = SimpleDirectoryReader(input_dir=path_documents).load_data()\n",
    "\n",
    "    rag_dataset_pandas = rag_dataset.to_pandas()\n",
    "    queries = rag_dataset_pandas[\"query\"]\n",
    "    reference_answers = rag_dataset_pandas[\"reference_answer\"]\n",
    "    \n",
    "    queries = queries[:20]\n",
    "    reference_answers = reference_answers[:20].to_list()\n",
    "\n",
    "    splitter = SentenceSplitter(chunk_size=512)\n",
    "    vector_index = VectorStoreIndex.from_documents(\n",
    "        documents, transformations=[splitter]\n",
    "    )\n",
    "\n",
    "    # ponemos todos los evaluadores que queremos usar aquí.\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"correctness\": correctness_gpt3_5,\n",
    "         \"faithfulness\": faithfulness_gpt3_5,\n",
    "         \"relevancy\": relevancy_gpt3_5,\n",
    "         \"semantic\": semantic},\n",
    "        workers=8,\n",
    "    )\n",
    "    \n",
    "    # he corregido una cosa de aquí del ejemplo, que no estaba (llm=llm), supongo que por error.\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        vector_index.as_query_engine(llm=llm),\n",
    "        queries=queries,\n",
    "        reference=reference_answers,\n",
    "    )\n",
    "    \n",
    "    total_results = combinar_diccionarios(total_results, eval_results)\n",
    "    print(f\"length of correctcness results: {len(total_results[\"correctness\"])}\")\n",
    "\n",
    "    print(f\"results of dataset {dataset_name} of embeddings model: {Settings.embed_model.model_name}\")\n",
    "    score_faithfulness = get_eval_results(\"faithfulness\", eval_results)\n",
    "    score_relevancy = get_eval_results(\"relevancy\", eval_results)\n",
    "    score_correctness = get_eval_results(\"correctness\", eval_results)\n",
    "    score_semantic = get_eval_results(\"semantic\", eval_results)\n",
    "\n",
    "print(f\"total results of all datasets of embeddings model: {Settings.embed_model.model_name}\")\n",
    "score_faithfulness = get_eval_results(\"faithfulness\", total_results)\n",
    "score_relevancy = get_eval_results(\"relevancy\", total_results)\n",
    "score_correctness = get_eval_results(\"correctness\", total_results)\n",
    "score_semantic = get_eval_results(\"semantic\", total_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ce1c4d1-d3bd-4faa-bfe3-116501df2b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['correctness', 'faithfulness', 'relevancy', 'semantic'])\n",
      "dict_keys(['query', 'contexts', 'response', 'passing', 'feedback', 'score', 'pairwise_source', 'invalid_result', 'invalid_reason'])\n",
      "True\n",
      "The author's first experience with programming was on an IBM 1401 in the basement of his junior high school. He used an early version of Fortran as the programming language. The biggest challenge he faced was not having any data to input into the program, which severely limited what he could do with the computer.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(eval_results.keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].dict().keys())\n",
    "\n",
    "print(eval_results[\"correctness\"][0].passing)\n",
    "print(eval_results[\"correctness\"][0].response)\n",
    "print(eval_results[\"correctness\"][0].contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96e8f924-9e46-4b80-b4c1-3c352209567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvaluationResult(query='In the essay, the author mentions his early experiences with programming. Describe the first computer he used for programming, the language he used, and the challenges he faced.', contexts=None, response=\"The author's first experience with programming was on an IBM 1401 in the basement of his junior high school. He used an early version of Fortran as the programming language. The biggest challenge he faced was not having any data to input into the program, which severely limited what he could do with the computer.\", passing=True, feedback=\"The generated answer provides relevant information about the first computer the author used for programming (IBM 1401), the programming language used (Fortran), and the challenges faced (lack of data input). The details are accurate and align closely with the reference answer, but it lacks some specific details such as the author's age when he started using the computer.\", score=4.5, pairwise_source=None, invalid_result=False, invalid_reason=None)]\n"
     ]
    }
   ],
   "source": [
    "print(eval_results[\"correctness\"][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876cabe1-3c56-4cd9-8da6-b051537a660a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b378de-c3af-4e8d-9b43-9b110efaa550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af3f2e-4bf5-4acb-8d7c-0e2289aaecb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a478b-b060-4288-b479-dba8fa694680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bc6dc-7a82-48a9-94f5-da7b0d906da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
