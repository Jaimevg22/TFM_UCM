{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama_index\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-llms-ollama\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools.query_engine import QueryEngineTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Verificar si la GPU está disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crear una lista con los nombres de los ficheros en el directorio \"transcripciones\"\n",
    "directory_path = \"C:/Users/gmsol/Desktop/videodescargas/transcripciones\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "# importante: installar ollama y levantar el modelo que quieras usar por linea de comandos (https://github.com/ollama/ollama?tab=readme-ov-file)\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0, device_map=device, temperature=0.1)\n",
    "# Settings.llm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.llm.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CmoGanarCualquierDiscusinFerMirallestxt', 'CmoPodra1TrillndeLeonesGanarlealSoltxt', 'Intel5000ExtremeTechUpgradetxt', 'UnaClaseconelDrdelSueotxt']\n"
     ]
    }
   ],
   "source": [
    "# Obtener una lista con los nombres de los ficheros en el directorio \"transcripciones\"\n",
    "file_list = os.listdir(directory_path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo: CmoGanarCualquierDiscusinFerMirallestxt\n",
      "C:/Users/gmsol/Desktop/videodescargas/transcripciones\\CmoGanarCualquierDiscusinFerMirallestxt\n",
      "Procesando archivo: C:/Users/gmsol/Desktop/videodescargas/transcripciones\\CmoGanarCualquierDiscusinFerMirallestxt\n",
      "Procesando archivo: CmoPodra1TrillndeLeonesGanarlealSoltxt\n",
      "C:/Users/gmsol/Desktop/videodescargas/transcripciones\\CmoPodra1TrillndeLeonesGanarlealSoltxt\n",
      "Procesando archivo: C:/Users/gmsol/Desktop/videodescargas/transcripciones\\CmoPodra1TrillndeLeonesGanarlealSoltxt\n",
      "Procesando archivo: Intel5000ExtremeTechUpgradetxt\n",
      "C:/Users/gmsol/Desktop/videodescargas/transcripciones\\Intel5000ExtremeTechUpgradetxt\n",
      "Procesando archivo: C:/Users/gmsol/Desktop/videodescargas/transcripciones\\Intel5000ExtremeTechUpgradetxt\n",
      "Procesando archivo: UnaClaseconelDrdelSueotxt\n",
      "C:/Users/gmsol/Desktop/videodescargas/transcripciones\\UnaClaseconelDrdelSueotxt\n",
      "Procesando archivo: C:/Users/gmsol/Desktop/videodescargas/transcripciones\\UnaClaseconelDrdelSueotxt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Función para crear un Query Engine a partir de un fichero\n",
    "def create_qe(file_path):\n",
    "    # Cargar el documento desde un archivo individual\n",
    "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    return index.as_query_engine()\n",
    "\n",
    "\n",
    "\n",
    "# Crear una lista para almacenar las herramientas\n",
    "query_engine_tools = []\n",
    "\n",
    "# Iterar sobre los archivos .txt en el directorio\n",
    "for file_name in os.listdir(directory_path):\n",
    "    print(f\"Procesando archivo: {file_name}\")\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    print(file_path)\n",
    "    \n",
    "    # Verificar si es un archivo .txt\n",
    "    \n",
    "    print(f\"Procesando archivo: {file_path}\")\n",
    "        \n",
    "        # Crear el Query Engine para el archivo\n",
    "    query_engine = create_qe(file_path)\n",
    "        \n",
    "        # Crear el QueryEngineTool para este archivo\n",
    "    query_engine_tool = QueryEngineTool.from_defaults(\n",
    "            query_engine=query_engine,\n",
    "            description=f\"Útil cuando se hacen preguntas sobre {file_name}\"\n",
    "        )\n",
    "        \n",
    "        # Agregar la herramienta a la lista\n",
    "    query_engine_tools.append(query_engine_tool)\n",
    "\n",
    "# Crear el RouterQueryEngine con todas las herramientas creadas dinámicamente\n",
    "router_qe = RouterQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Sol es una bola con un radio de 700.000 kilómetros y una masa de 2 quintillones de kilos, mientras que un trillón de leones formaría una esfera con un radio de unos 400 km y un peso de 200 trillones de kilos. A pesar del tamaño considerable de la bola de leones, su energía sería insuficiente para destruir el Sol, incluso si se lanzara a una velocidad cercana a la velocidad de la luz. Por lo tanto, es probable que el Sol ganara en esta batalla.\n"
     ]
    }
   ],
   "source": [
    "question = \"Quien ganaria, los leones o el sol?\"\n",
    "prompt =f'''Tienes acceso a transcripciones de videos de youtube.\n",
    "            Se te hará una pregunta y debes responder basándote exclusivamente en la información proporcionada en las transcripciones. Si no puedes responder la pregunta utilizando estos datos, indica que la información no está disponible.\n",
    "\n",
    "            Responde de manera educada, estructurada y concisa cuando corresponda. Si la pregunta requiere una respuesta breve, sé claro y directo. Asegúrate de:\n",
    "\n",
    "            No inventar información adicional ni suponer nada que no esté claramente expresado en las transcripciones.\n",
    "            Responder solo utilizando la información proporcionada, sin hacer inferencias más allá del contenido.\n",
    "            Estructurar la respuesta de manera clara y fácil de entender.\n",
    "            Mantener un tono respetuoso y profesional en todo momento.\n",
    "            \n",
    "            PREGUNTA:\n",
    "            {question}\n",
    "            '''\n",
    "response = router_qe.query(question)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'C:\\\\Users\\\\gmsol\\\\Desktop\\\\videodescargas\\\\transcripciones\\\\CmoPodra1TrillndeLeonesGanarlealSoltxt',\n",
       " 'file_name': 'CmoPodra1TrillndeLeonesGanarlealSoltxt',\n",
       " 'file_size': 11994,\n",
       " 'creation_date': '2024-09-13',\n",
       " 'last_modified_date': '2024-09-13'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0].metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
